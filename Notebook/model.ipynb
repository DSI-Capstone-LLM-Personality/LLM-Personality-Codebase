{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- #\n",
    "# NOTEBOOK MPI EXPERIMENT #\n",
    "# AUTHOR: XIAOYANG SONG   #\n",
    "# ----------------------- #\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that on MacBook, these downloaded models will be stored in `User/username/.cache/huggingface/hub/`. To remove, go to that directory and then `rm -r {model-name}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tabulate import tabulate\n",
    "sys.path.append('../')\n",
    "sys.path.append('')\n",
    "from MPI.mpi import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| outputs.last_hidden_state.shape: torch.Size([1, 8, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "ic| inputs: {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      "             'input_ids': tensor([[  101,  2445,  1037,  4861,  1997,  2017,  1024,  1000,  2017,  4737,\n",
      "                      2055,  2477,  1012,  1000,  3531,  5454,  3599,  2028,  2013,  1996,\n",
      "                      2206,  7047,  2000,  6709,  2129, 14125,  2023,  4861,  5577,  2017,\n",
      "                      1012,  7047,  1024,  2200,  8321, 17844,  8321,  4445,  8321,  4496,\n",
      "                     24949, 17844, 24949,  2200, 24949,  6998,  1024,   102]]),\n",
      "             'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "ic| inputs.input_ids.shape: torch.Size([1, 48])\n",
      "ic| tokenizer.decode(inputs.input_ids[0][-8:-1]): 'inaccurate moderately inaccurate very inaccurate answers :'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaForCausalLM, AutoConfig, AlbertForPreTraining\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "# config = AutoConfig.from_pretrained(\"roberta-base\")\n",
    "# config.is_decoder = False\n",
    "# config.is_decoder=True\n",
    "\n",
    "model = BertLMHeadModel.from_pretrained(\"bert-base-uncased\")\n",
    "# model = RobertaForCausalLM.from_pretrained(\"roberta-base\")\n",
    "# model = AlbertForPreTraining.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "item = \"worry about things\"\n",
    "eg_q = MPI_TEMPLATE.format(item=item, template=MPI_PROMPT_EXACT) + ordered_lst_to_str(MPI_DESC)\n",
    "inputs = tokenizer(eg_q, return_tensors=\"pt\")\n",
    "# ic(len(choice.input_ids[0,1:-1]))\n",
    "# ic(tokenizer.decode(choice.input_ids[0]))\n",
    "# ic(tokenizer.decode(choice.input_ids[0][1:-1]))\n",
    "# inputs = tokenizer(\"Hello, my dog is cute, I love dog\", return_tensors=\"pt\")\n",
    "ic(inputs)\n",
    "ic(inputs.input_ids.shape)\n",
    "ic(tokenizer.decode(inputs.input_ids[0][-8:-1]))\n",
    "outputs = model(**inputs)\n",
    "prediction_logits = outputs.logits\n",
    "# prediction_logits = outputs.prediction_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in inputs.input_ids[0]:\n",
    "    ic(tokenizer.decode(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| prediction_logits.squeeze().shape: torch.Size([48, 30522])\n",
      "ic| inputs.input_ids[0].shape: torch.Size([48])\n",
      "ic| prob.shape: torch.Size([48, 30522])\n",
      "ic| masked_prob.shape: torch.Size([48])\n",
      "ic| masked_prob: tensor([6.8377e-07, 3.5591e-04, 9.9979e-01, 9.9838e-01, 9.9969e-01, 9.9081e-01,\n",
      "                         9.9981e-01, 1.0000e+00, 9.9798e-01, 9.6764e-01, 9.4483e-01, 9.9553e-01,\n",
      "                         9.9999e-01, 1.0000e+00, 9.8293e-01, 9.6974e-01, 9.2508e-01, 9.9995e-01,\n",
      "                         1.3080e-04, 9.9953e-01, 9.9817e-01, 9.9400e-01, 9.9994e-01, 8.1662e-01,\n",
      "                         1.0000e+00, 9.9145e-01, 9.9157e-01, 9.9888e-01, 9.7285e-01, 9.9891e-01,\n",
      "                         9.9999e-01, 9.9114e-01, 1.0000e+00, 9.7876e-01, 8.7856e-01, 8.1290e-01,\n",
      "                         9.5235e-01, 9.9554e-01, 9.4846e-01, 9.9537e-01, 9.1612e-01, 7.6658e-01,\n",
      "                         9.3284e-01, 9.5950e-01, 9.3008e-01, 9.8506e-01, 9.9947e-01, 4.1430e-10],\n",
      "                        grad_fn=<IndexBackward0>)\n",
      "ic| idx: tensor([ 1012,  1012,  1037,  4861,  1997,  2017,  1024,  1000,  2017,  4737,\n",
      "                  2055,  2477,  1012,  1000,  3531,  5454,  3599,  2028,  1997,  1996,\n",
      "                  2206,  7047,  2000,  6709,  2129, 14125,  2023,  4861,  5577,  2017,\n",
      "                  1012,  7047,  1024,  2200,  8321, 17844,  8321,  4445,  8321,  4496,\n",
      "                 24949, 17844, 24949,  2200, 24949,  6998,  1024,  1012])\n",
      "ic| tokenizer.decode(idx): ('.. a statement of you : \" you worry about things. \" please choose exactly '\n",
      "                            'one of the following options to identify how accurately this statement '\n",
      "                            'describes you. options : very accurate moderately accurate neither accurate '\n",
      "                            'nor inaccurate moderately inaccurate very inaccurate answers :.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.. a statement of you : \" you worry about things. \" please choose exactly one of the following options to identify how accurately this statement describes you. options : very accurate moderately accurate neither accurate nor inaccurate moderately inaccurate very inaccurate answers :.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(prediction_logits.squeeze().shape)\n",
    "ic(inputs.input_ids[0].shape)\n",
    "prob = torch.softmax(prediction_logits.squeeze(), dim=-1)\n",
    "ic(prob.shape)\n",
    "masked_prob = prob[np.arange(inputs.input_ids[0].shape[0]), inputs.input_ids[0]]\n",
    "ic(masked_prob.shape)\n",
    "ic(masked_prob)\n",
    "idx = torch.max(prob, dim=-1)[1]\n",
    "ic(idx)\n",
    "ic(tokenizer.decode(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| eg.shape: torch.Size([30522])\n",
      "ic| tokenizer.decode([idx]): '.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eg = prob[1]\n",
    "ic(eg.shape)\n",
    "plt.plot(np.arange(len(eg)), eg.detach().cpu())\n",
    "idx = torch.argmax(eg)\n",
    "ic(tokenizer.decode([idx]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT (Constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(choice.input_ids[0,:]): 8\n",
      "ic| tokenizer.decode(choice.input_ids[0]): '</s>(A). Very Inaccurate'\n",
      "ic| tokenizer.decode(choice.input_ids[0]): '</s>(A). Very Inaccurate'\n",
      "ic| inputs: {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      "             'input_ids': tensor([[ 6211,    10,   445,     9,    47,     6,    47,  4022,    59,   383,\n",
      "                         6,    99,   109,    47,   206,    35,  1437, 50118, 25101,  5438,\n",
      "                     23412,  1437, 50118, 38451,  7223,  5438, 23412,  1437,  1437,  1437,\n",
      "                      1437,  1437,  1437,  1437,  1437,  1437,  1437, 50118, 39254,  5438,\n",
      "                     23412,  6567,    96,  7904, 23412,  1437, 50118, 38451,  7223,    96,\n",
      "                      7904, 23412,  1437, 50118, 12178,    96,  7904, 23412,  1437, 50118,\n",
      "                     31652,    35,  9081,  5438, 23412,  6567,    96,  7904, 23412]])}\n",
      "ic| tokenizer.decode(inputs.input_ids[0][-7:]): ' Neither Accurate Nor Inaccurate'\n",
      "ic| prediction_logits.shape: torch.Size([1, 69, 50272])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "#     tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#     model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    # tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "    # model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "    from transformers import AutoTokenizer, OPTForCausalLM, GPTNeoXTokenizerFast\n",
    "\n",
    "    model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "    # tokenizer = GPTNeoXTokenizerFast.from_pretrained(\"gpt2\")\n",
    "    item = \"worry about things\"\n",
    "    # eg_q = MPI_TEMPLATE.format(item=item, template=MPI_PROMPT_EXACT) + ordered_lst_to_str(MPI_DESC)\n",
    "    eg_q = \" Given a statement of you, you worry about things, what do you think: \\nVery Accurate \\n Moderately Accurate\\\n",
    "          \\nNeither Accurate Nor Inaccurate \\n Moderately Inaccurate \\n Very Inaccurate \\n Answer: Neither Accurate Nor Inaccurate\"\n",
    "    inputs = tokenizer(eg_q, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    choice = tokenizer(\"(A). Very Inaccurate\", return_tensors='pt')\n",
    "    ic(len(choice.input_ids[0,:]))\n",
    "    ic(tokenizer.decode(choice.input_ids[0]))\n",
    "    ic(tokenizer.decode(choice.input_ids[0]))\n",
    "    # inputs = tokenizer(\"Hello, my dog is cute, I love dog\", return_tensors=\"pt\")\n",
    "    ic(inputs)\n",
    "    ic(tokenizer.decode(inputs.input_ids[0][-7:]))\n",
    "    outputs = model(**inputs, labels=inputs.input_ids)\n",
    "\n",
    "    prediction_logits = outputs.logits\n",
    "    ic(prediction_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Nor', [tensor(6567)])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_critical_word(tokenizer, encoded_seq):\n",
    "    '''gives the tokenization of the last word in the sequence'''\n",
    "\n",
    "    critical_word = ''\n",
    "    tokens_indices = []#tokenized indices of the sequence\n",
    "\n",
    "    for token in torch.flip(encoded_seq[0], dims=(0,)):\n",
    "        token_string = tokenizer.convert_ids_to_tokens([token])[0]\n",
    "        tokens_indices.insert(0, token)\n",
    "        if 'Ġ' in token_string:\n",
    "            critical_word = token_string[1:] + critical_word\n",
    "            break\n",
    "        else:\n",
    "            critical_word = token_string + critical_word\n",
    "\n",
    "    return critical_word, tokens_indices\n",
    "print(len(inputs.input_ids[0]))\n",
    "find_critical_word(tokenizer, inputs.input_ids)\n",
    "find_critical_word(tokenizer, inputs.input_ids[:,:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.language_model import find_critical_phrase\n",
    "toi = find_critical_phrase(tokenizer, inputs.input_ids, \"Neither Accurate Nor Inaccurate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| toi: [tensor(9081),\n",
      "          tensor(5438),\n",
      "          tensor(23412),\n",
      "          tensor(6567),\n",
      "          tensor(96),\n",
      "          tensor(7904),\n",
      "          tensor(23412)]\n",
      "ic| len(toi): 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Neither Accurate Nor Inaccurate'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(toi)\n",
    "ic(len(toi))\n",
    "tokenizer.decode(toi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Given'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([11259])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.softmax(prediction_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2605e+01, -8.6148e+00, -7.2170e+00, -9.2804e+00, -7.7124e+00,\n",
       "         -9.4323e+00, -9.5413e+00, -1.5722e+01, -1.2558e+01, -8.0769e+00,\n",
       "         -8.5154e+00, -8.7695e+00, -8.0322e-03, -1.1713e+01, -1.0143e+01,\n",
       "         -6.6645e+00, -6.5709e+00, -7.7643e+00, -8.5234e+00, -8.1259e+00,\n",
       "         -9.7503e+00, -8.9154e+00, -1.2273e+01, -9.5984e+00, -1.0543e+01,\n",
       "         -1.0452e+01, -1.0183e+01, -1.3269e+01, -9.5586e+00, -1.0391e+01,\n",
       "         -4.4546e+00, -1.4454e+01, -7.4719e+00, -5.5233e+00, -7.7713e+00,\n",
       "         -8.5504e+00, -1.5832e+01, -1.6264e+01, -2.9881e+00, -7.5690e+00,\n",
       "         -1.3130e+01, -1.3089e+01, -1.8006e+01, -2.0978e+01, -1.6619e+01,\n",
       "         -3.8818e+00, -8.6653e+00, -1.5502e+01, -1.8874e+01, -1.6678e+01,\n",
       "         -8.2146e+00, -1.2291e+01, -1.5442e+01, -1.7172e+01, -4.3043e+00,\n",
       "         -9.1362e+00, -1.7086e+01, -1.6256e+01, -1.8770e+01, -1.5819e+01,\n",
       "         -1.3015e+01, -1.7177e+01, -3.5357e+00, -8.5808e+00, -1.1716e+01,\n",
       "         -1.5697e+01, -1.3735e+01, -1.7101e+01, -3.8403e+00, -8.2599e+00,\n",
       "         -1.0196e+01, -1.0607e+01, -1.3587e+01, -8.1694e+00, -4.2287e+00]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(probs[0][np.arange(75), inputs.input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 75, 50257])\n",
      "tensor([0.0910, 0.0164, 0.2488, 0.0884, 0.0787, 0.1725, 0.1949, 0.1479, 0.5004,\n",
      "        0.1284, 0.2469, 0.2123, 0.9920, 0.0959, 0.1381, 0.5110, 0.3272, 0.4288,\n",
      "        0.7103, 0.5654, 0.4188, 0.0474, 0.2349, 0.4183, 0.5692, 0.1481, 0.4616,\n",
      "        0.3661, 0.4033, 0.4302, 0.0685, 0.3258, 0.2984, 0.4649, 0.0446, 0.0496,\n",
      "        0.9918, 0.1945, 0.2685, 0.0558, 0.3687, 0.7395, 0.9289, 0.9997, 0.7438,\n",
      "        0.7605, 0.0372, 0.4855, 0.9976, 0.6723, 0.6461, 0.9215, 0.9986, 0.5815,\n",
      "        0.8307, 0.0418, 0.9208, 0.9870, 0.7116, 0.9987, 0.9979, 0.5649, 0.7430,\n",
      "        0.0722, 0.7366, 0.9976, 0.9996, 0.5846, 0.7948, 0.1239, 0.1914, 0.2328,\n",
      "        0.3354, 0.2692, 0.7966])\n",
      "tensor([  262,  1178,   588,  1109,   290,   198,    40,   389,   546,   262,\n",
      "          588,   198,   198,   836,   534,   644,   286,   262,  1351,  1351,\n",
      "           25,  1037,   534,   345,   345,  2643,   318,   345,    13,   198,\n",
      "         1639,   352,   198,  1849,  1639, 28511, 15537,   198,   198,  1639,\n",
      "        21620,  1286,  6366, 15537,   220,   198, 16371,  6366, 15537,  4249,\n",
      "         6366,  4134, 15537,   220,   198,  1639,   263,  1286,  6366,  4134,\n",
      "        15537,   220,   198,  5841,  6366,  4134, 15537,   220,   198,  5841,\n",
      "         5841,  6366,    25,   220,   198])\n",
      " the few like fact and\n",
      "I are about the like\n",
      "\n",
      " don your what of the list list: help your you you statement is you.\n",
      "You 1\n",
      " You Importanturate\n",
      "\n",
      "Youerateately Accurate \n",
      "Very Accurate nor Accaccurate \n",
      "Youerately Accaccurate \n",
      "Mod Accaccurate \n",
      "ModMod Acc: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "probs = torch.softmax(prediction_logits, dim=-1)\n",
    "print(probs.shape)\n",
    "print(torch.max(probs, dim=-1)[0].squeeze())\n",
    "idxs = torch.max(probs, dim=-1)[1].squeeze()\n",
    "print(idxs)\n",
    "print(tokenizer.decode(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| eg.shape: torch.Size([50257])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Hello World.\n",
      "Answer: (A). Very Inaccurate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| idx: tensor(11)\n",
      "ic| tokenizer.decode([idx]): ','\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "print(\"Hello Hello World.\\nAnswer: (A). Very Inaccurate\")\n",
    "eg = probs[0][0]\n",
    "ic(eg.shape)\n",
    "plt.plot(np.arange(len(eg)), eg.detach().cpu())\n",
    "idx = torch.argmax(eg)\n",
    "ic(idx)\n",
    "ic(tokenizer.decode([idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| -loss: tensor(-3.9902, grad_fn=<NegBackward0>)\n",
      "ic| logits.shape: torch.Size([1, 6, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 50257])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "ic(-loss)\n",
    "ic(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| logits.shape: torch.Size([1, 6, 50257])\n",
      "ic| probs.shape: torch.Size([6])\n",
      "ic| torch.sum(torch.log(probs)): tensor(-53.3722, grad_fn=<SumBackward0>)\n",
      "ic| idxs: tensor([  11,  314, 1438,  318,  257,   13])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I name is a.\n",
      "Hello, my dog is cute\n"
     ]
    }
   ],
   "source": [
    "ic(logits.shape)\n",
    "idxs = torch.argmax(logits.squeeze(), dim=-1)\n",
    "probs = torch.softmax(logits.squeeze(), dim=-1)[np.arange(6), inputs.input_ids[0]]\n",
    "ic(probs.shape)\n",
    "ic(torch.sum(torch.log(probs)))\n",
    "ic(idxs)\n",
    "print(tokenizer.decode(idxs))\n",
    "print(tokenizer.decode(inputs.input_ids[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT (Open Vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.language_model import *\n",
    "from Model.template import *\n",
    "# version = 'gpt2'\n",
    "version = 'gpt2-large'\n",
    "tokenizer = TOKENIZER['GPT2'].from_pretrained(version)\n",
    "model = MODEL['Open-Vocab']['GPT2'].from_pretrained(version)\n",
    "# model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "                                           \n",
    "with torch.no_grad():\n",
    "    # Text completion\n",
    "    item = \"worry about things\"\n",
    "    eg_q = MPI_TEMPLATE.format(item=item, template=MPI_PROMPT_EXACT) + ordered_lst_to_str(MPI_DESC)\n",
    "    # # print(eg_q)\n",
    "    inputs = tokenizer(eg_q, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids\n",
    "    response = model.generate(input_ids, do_sample=True, top_p=0.95, temperature=0.1,\n",
    "                                    num_return_sequences=1, early_stopping=True, max_new_tokens=70)\n",
    "    output = tokenizer.decode(response[0])\n",
    "print(output)\n",
    "print(len(response[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline, set_seed\n",
    "item = \"worry about things\"\n",
    "eg_q = MPI_TEMPLATE.format(item=item, template=MPI_PROMPT_EXACT) + ordered_lst_to_str(MPI_DESC)\n",
    "generator = pipeline('text-generation', model='gpt2-large')\n",
    "transformers.set_seed(42)\n",
    "print(generator(eg_q, max_length=200, top_p = 0.95, temperature=0.10, num_return_sequences=1)[0]['generated_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT2-NEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast, AutoTokenizer\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Text completion\n",
    "    item = \"worry about things\"\n",
    "    eg_q = MPI_TEMPLATE.format(item=item, template=MPI_PROMPT_EXACT) + ordered_lst_to_str(MPI_DESC)\n",
    "    # print(eg_q)\n",
    "    inputs = tokenizer(eg_q, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids\n",
    "    response = model.generate(input_ids,top_p=0.95, temperature=0.1,\n",
    "                                   max_new_tokens=70)\n",
    "    output = tokenizer.decode(response[0])\n",
    "print(output)\n",
    "print(len(response[0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-3\n",
    "\n",
    "API reference: [Check this website out!](https://platform.openai.com/docs/api-reference/completions/create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Model model id=text-davinci-002 at 0x7fbb7c5f9310> JSON: {\n",
       "  \"created\": 1649880484,\n",
       "  \"id\": \"text-davinci-002\",\n",
       "  \"object\": \"model\",\n",
       "  \"owned_by\": \"openai\",\n",
       "  \"parent\": null,\n",
       "  \"permission\": [\n",
       "    {\n",
       "      \"allow_create_engine\": false,\n",
       "      \"allow_fine_tuning\": false,\n",
       "      \"allow_logprobs\": true,\n",
       "      \"allow_sampling\": true,\n",
       "      \"allow_search_indices\": false,\n",
       "      \"allow_view\": true,\n",
       "      \"created\": 1679355287,\n",
       "      \"group\": null,\n",
       "      \"id\": \"modelperm-l4EU6QlN1HcS0so0jU16kyg8\",\n",
       "      \"is_blocking\": false,\n",
       "      \"object\": \"model_permission\",\n",
       "      \"organization\": \"*\"\n",
       "    }\n",
       "  ],\n",
       "  \"root\": \"text-davinci-002\"\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = read_api_key(\"../../\", 'kiyan')\n",
    "openai.Model.retrieve(\"text-davinci-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text_offset\": [\n",
      "    0,\n",
      "    3,\n",
      "    7,\n",
      "    9,\n",
      "    17,\n",
      "    24,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    30,\n",
      "    31,\n",
      "    33,\n",
      "    36,\n",
      "    38,\n",
      "    46,\n",
      "    53\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    null,\n",
      "    -0.5484283,\n",
      "    -2.3484318,\n",
      "    -11.56091,\n",
      "    -0.3344945,\n",
      "    -0.09188025,\n",
      "    -0.0015265809,\n",
      "    -1.9621319e-05,\n",
      "    -0.4918428,\n",
      "    -0.11833226,\n",
      "    -0.002766698,\n",
      "    -0.967397,\n",
      "    -0.5009338,\n",
      "    -0.04270428,\n",
      "    -1.843017e-05,\n",
      "    -0.0022791522\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \"Are\",\n",
      "    \" you\",\n",
      "    \" a\",\n",
      "    \" helpful\",\n",
      "    \" person\",\n",
      "    \"?\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"Yes\",\n",
      "    \",\",\n",
      "    \" I\",\n",
      "    \" am\",\n",
      "    \" a\",\n",
      "    \" helpful\",\n",
      "    \" person\",\n",
      "    \".\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    null,\n",
      "    {\n",
      "      \" I\": -4.09474,\n",
      "      \" there\": -3.738496,\n",
      "      \" you\": -0.5484283,\n",
      "      \"E\": -3.945342,\n",
      "      \"as\": -2.8397064\n",
      "    },\n",
      "    {\n",
      "      \" a\": -2.3484318,\n",
      "      \" going\": -3.450221,\n",
      "      \" looking\": -2.8057222,\n",
      "      \" ready\": -3.3230975,\n",
      "      \" sure\": -1.8694134\n",
      "    },\n",
      "    {\n",
      "      \" fan\": -2.0240815,\n",
      "      \" good\": -3.8889964,\n",
      "      \" man\": -3.9106426,\n",
      "      \" member\": -3.4057312,\n",
      "      \" student\": -3.8714497\n",
      "    },\n",
      "    {\n",
      "      \" and\": -3.8716815,\n",
      "      \" individual\": -4.2070265,\n",
      "      \" or\": -4.286822,\n",
      "      \" person\": -0.3344945,\n",
      "      \",\": -3.993666\n",
      "    },\n",
      "    {\n",
      "      \"\\n\": -3.3526375,\n",
      "      \" or\": -5.390807,\n",
      "      \",\": -5.2420874,\n",
      "      \"?\": -0.09188025,\n",
      "      \"?\\\"\": -4.2187634\n",
      "    },\n",
      "    {\n",
      "      \"\\n\": -0.0015265809,\n",
      "      \"\\n\\n\": -7.807446,\n",
      "      \" Yes\": -8.35794,\n",
      "      \"I\": -8.156871,\n",
      "      \"Yes\": -8.013374\n",
      "    },\n",
      "    {\n",
      "      \"\\n\": -1.9621319e-05,\n",
      "      \"\\n\\n\": -14.247762,\n",
      "      \"<|endoftext|>\": -13.680687,\n",
      "      \"I\": -12.225112,\n",
      "      \"Yes\": -11.563834\n",
      "    },\n",
      "    {\n",
      "      \"I\": -1.0654407,\n",
      "      \"In\": -5.3569093,\n",
      "      \"Most\": -5.413487,\n",
      "      \"This\": -5.6663046,\n",
      "      \"Yes\": -0.4918428\n",
      "    },\n",
      "    {\n",
      "      \" I\": -6.4746933,\n",
      "      \"!\": -9.12177,\n",
      "      \",\": -0.11833226,\n",
      "      \".\": -3.7328656,\n",
      "      \"<|endoftext|>\": -2.4551957\n",
      "    },\n",
      "    {\n",
      "      \" I\": -0.002766698,\n",
      "      \" generally\": -7.187319,\n",
      "      \" helping\": -8.581952,\n",
      "      \" typically\": -8.334146,\n",
      "      \" usually\": -8.500663\n",
      "    },\n",
      "    {\n",
      "      \" am\": -0.967397,\n",
      "      \" generally\": -3.5827951,\n",
      "      \" like\": -1.542928,\n",
      "      \" try\": -1.5883944,\n",
      "      \"'m\": -3.8627315\n",
      "    },\n",
      "    {\n",
      "      \" a\": -0.5009338,\n",
      "      \" generally\": -3.955027,\n",
      "      \" helpful\": -1.7348043,\n",
      "      \" very\": -3.8509004,\n",
      "      \".\": -2.091243\n",
      "    },\n",
      "    {\n",
      "      \" Helpful\": -8.901643,\n",
      "      \" generally\": -8.727214,\n",
      "      \" helper\": -8.966566,\n",
      "      \" helpful\": -0.04270428,\n",
      "      \" very\": -3.1924405\n",
      "    },\n",
      "    {\n",
      "      \" Person\": -13.586044,\n",
      "      \" and\": -11.410199,\n",
      "      \" per\": -13.268677,\n",
      "      \" pers\": -14.0090475,\n",
      "      \" person\": -1.843017e-05\n",
      "    },\n",
      "    {\n",
      "      \" and\": -6.315359,\n",
      "      \" by\": -9.372658,\n",
      "      \",\": -9.415296,\n",
      "      \".\": -0.0022791522,\n",
      "      \"<|endoftext|>\": -8.821958\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from util.utils import *\n",
    "from icecream import ic\n",
    "set_seed(2023)\n",
    "# Make sure to redirect to the correct path\n",
    "# I don't know why but it shows that Kiyan's api key is not correct.\n",
    "openai.api_key = read_api_key(\"../../\", 'kiyan')\n",
    "# openai.api_key = read_api_key(\"../../\", 'xysong')\n",
    "# eg_q = \"Please use a word to describe yourself.\"\n",
    "item = \"worry about things\"\n",
    "item = \"Are you a helpful person?\"\n",
    "response = openai.Completion.create(engine=\"text-davinci-002\", prompt=item, echo=True, temperature=0.1, max_tokens=100, top_p=0.95, logprobs=5)\n",
    "# print(response['choices'][0]['text'])\n",
    "# print(response.keys())\n",
    "print(response['choices'][0]['logprobs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_completion\n",
      "1681003755\n",
      "text-davinci-002\n",
      "{\n",
      "  \"completion_tokens\": 7,\n",
      "  \"prompt_tokens\": 75,\n",
      "  \"total_tokens\": 82\n",
      "}\n",
      "1\n",
      "dict_keys(['text', 'index', 'logprobs', 'finish_reason'])\n"
     ]
    }
   ],
   "source": [
    "print(response['object'])\n",
    "print(response['created'])\n",
    "print(response['model'])\n",
    "print(response['usage'])\n",
    "print(len(response['choices']))\n",
    "print(response['choices'][0].keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(inputs.input_ids): 1\n",
      "ic| inputs.input_ids: tensor([[    0, 18377,    10,   445,     9,    47,    35,    22,  1185,   370,\n",
      "                                4022,    59,   383,    72, 50118,  6715,  2807,  2230,    65,    31,\n",
      "                                   5,   511,  1735,     7,  3058,   141, 12775,    42,   445,  7448,\n",
      "                                  47,     4, 50118, 47261,    35,  1437, 50118, 25101,  5438, 23412,\n",
      "                                1437, 50118, 30597,   254,  7223,  5438, 23412,  1437, 50118, 39254,\n",
      "                                5438, 23412,  6567,    96,  7904, 23412,  1437, 50118, 30597,   254,\n",
      "                                7223,    96,  7904, 23412,  1437, 50118, 25101,    96,  7904, 23412,\n",
      "                                1437, 50118, 50118,  4688, 48792,    35,  1437,     2]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s><s><s>Given a statement of you: \"You You worry about things,\" choose one of the following options to identify how accurately this statement describes you. Options: Very Accurate, Very Inaccurate, Moderately Accurate or Neither Accurate. Answers:  \"You worry aboutThings\" \"Very Accurate\" or \"Very Inaccurately\"</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "item = \"You worry about things\"\n",
    "eg_q = MPI_TEMPLATE.format(item=item, template=MPI_PROMPT_EXACT) + ordered_lst_to_str(MPI_DESC)\n",
    "inputs = tokenizer(eg_q, return_tensors=\"pt\")\n",
    "ic(len(inputs.input_ids))\n",
    "ic(inputs.input_ids)\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], top_p=0.95,temperature=0.10, max_new_tokens=100)\n",
    "print(tokenizer.decode(summary_ids[0]))\n",
    "# tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "generator = pipeline('text-generation', model=\"facebook/bart-large-cnn\")\n",
    "# generator = pipeline('text-generation', model=\"gpt2-large\")\n",
    "transformers.set_seed(42)\n",
    "# print(generator(\"Who is Issac Newton?\", max_length=100, top_p = 0.95, temperature=0.10, num_return_sequences=1)[0]['generated_text'])\n",
    "print(generator(\"Who is Issac Newton?\", max_length=100, top_p = 0.95, temperature=0.10, num_return_sequences=1)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], dtype=torch.int64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = summary_ids[0][78:]\n",
    "print(response)\n",
    "answers = tokenizer.decode(response)\n",
    "print(answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Given a statement of you: you make friend easily. What do you think?\\n Very Accurate\\n Moderately Accurate\\nNeither Accurate Nor Inaccurate\\nModerately Inaccurate\\nVery Inaccurate\\nAnswer: \\xa0I don't know. I'm not sure. But I do know that it's a good idea to be honest with yourself about what you're doing. If you have any questions, please feel free to reach out to me.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "\n",
    "# model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "from model.language_model import *\n",
    "from template.templates import *\n",
    "# version = 'gpt2'\n",
    "version = 'gpt2'\n",
    "tokenizer = TOKENIZER['GPT2'].from_pretrained(version)\n",
    "model = MODEL['Open-Vocab']['GPT2'].from_pretrained(version, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# prompt = \"Hey, are you consciours? Can you talk to me?\"\n",
    "prompt = \"Given a statement of you: you make friend easily. What do you think?\\n Very Accurate\\n Moderately Accurate\\nNeither Accurate Nor Inaccurate\\nModerately Inaccurate\\nVery Inaccurate\\nAnswer: \"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_new_tokens=80,num_beams = 5, \n",
    "    no_repeat_ngram_size = 2, \n",
    "    num_return_sequences = 5, \n",
    "    early_stopping = True)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
