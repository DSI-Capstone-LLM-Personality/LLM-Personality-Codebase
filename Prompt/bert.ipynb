{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from bert_mpi import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from bert_mpi import *\n",
    "from transformers import AutoTokenizer, BertForMultipleChoice, BertLMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertLMHeadModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| prob.shape: torch.Size([67, 30522])\n",
      "ic| torch.sum(torch.log(logits)): tensor(-6.6448, grad_fn=<SumBackward0>)\n",
      "ic| logits: tensor([0.0278, 0.2255, 0.9994, 0.9981, 0.9998, 0.9954, 0.9995, 1.0000, 0.9994,\n",
      "                    0.9979, 0.9877, 0.9978, 1.0000, 1.0000, 0.9717, 0.8981, 0.8578, 0.9991,\n",
      "                    0.9992, 0.9904, 0.9999, 0.8551, 1.0000, 0.9916, 0.9933, 0.9986, 0.9736,\n",
      "                    0.9981, 1.0000, 0.9947, 1.0000, 0.9998, 0.9999, 1.0000, 1.0000, 0.9971,\n",
      "                    0.9782, 1.0000, 0.9990, 1.0000, 0.9999, 0.9404, 0.9561, 1.0000, 0.9912,\n",
      "                    0.9978, 1.0000, 0.9983, 0.9956, 0.9996, 0.6608, 1.0000, 0.9995, 1.0000,\n",
      "                    1.0000, 0.9520, 0.7708, 1.0000, 0.9990, 0.9816, 1.0000, 0.9985, 0.9171,\n",
      "                    0.9871, 1.0000, 0.9685, 0.9895], grad_fn=<MaxBackward0>)\n",
      "ic| logits.shape: torch.Size([67])\n",
      "ic| loss: None\n"
     ]
    }
   ],
   "source": [
    "t1 = \"Make friends easily\"\n",
    "t2 = \"Trust other people\"\n",
    "t3 = \"Have difficulty imagining things\"\n",
    "t_lst = [t1, t2, t3]\n",
    "prompt_lst = [prepare_mpi_questions(t) for t in t_lst]\n",
    "# ic(prompt_lst[0])\n",
    "inputs = tokenizer(prompt_lst[0] +  \"A\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "outputs = model(**inputs)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "# choice0 = \"It is eaten with a fork and a knife.\"\n",
    "# choice1 = \"It is eaten while held in the hand.\"\n",
    "# labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "# statement = \"Trust others.\"\n",
    "t1 = \"Make friends easily\"\n",
    "t2 = \"Trust other people\"\n",
    "t3 = \"Have difficulty imagining things\"\n",
    "t_lst = [t1, t2, t3]\n",
    "prompt_lst = [prepare_mpi_questions(t) for t in t_lst]\n",
    "\n",
    "\n",
    "# prompt_lst = []\n",
    "for prompt in prompt_lst:\n",
    "    encoding = tokenizer([prompt]*5, ['A', 'B', 'C', 'D', 'E'], return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()})  # batch size is 1\n",
    "\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    ic(logits)\n",
    "    print(MPI_IDX_TO_KEY[torch.argmax(logits.squeeze()).item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "ic| self.questions.shape: (120,)\n",
      "ic| Counter(col): Counter({'N': 24, 'E': 24, 'O': 24, 'A': 24, 'C': 24})\n",
      "ic"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 120 multiple choice questions in total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| len(Counter(col)): 5\n",
      "ic| Counter(col): Counter({1: 65, -1: 55})\n",
      "ic| len(Counter(col)): 2\n",
      "  0%|          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a statement of you: You Worry about things. Please choose from the following options to identify how accurately this statement describes you.\n",
      "Options: (A). Very Accurate \n",
      "(B). Moderately Accurate \n",
      "(C). Neither Accurate Nor Inaccurate \n",
      "(D). Moderately Inaccurate \n",
      "(E). Very Inaccurate \n",
      "Answers: A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f\"{choice}: {torch.sum(torch.log(prob)) - ll}|{ll}\": 'A: -7.622119426727295|-0.03606022149324417'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a statement of you: You Worry about things. Please choose from the following options to identify how accurately this statement describes you.\n",
      "Options: (A). Very Accurate \n",
      "(B). Moderately Accurate \n",
      "(C). Neither Accurate Nor Inaccurate \n",
      "(D). Moderately Inaccurate \n",
      "(E). Very Inaccurate \n",
      "Answers: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f\"{choice}: {torch.sum(torch.log(prob)) - ll}|{ll}\": 'B: -7.4361958503723145|-0.027135083451867104'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a statement of you: You Worry about things. Please choose from the following options to identify how accurately this statement describes you.\n",
      "Options: (A). Very Accurate \n",
      "(B). Moderately Accurate \n",
      "(C). Neither Accurate Nor Inaccurate \n",
      "(D). Moderately Inaccurate \n",
      "(E). Very Inaccurate \n",
      "Answers: C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f\"{choice}: {torch.sum(torch.log(prob)) - ll}|{ll}\": 'C: -7.625903606414795|-0.028094688430428505'\n",
      "  0%|          | 0/120 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a statement of you: You Worry about things. Please choose from the following options to identify how accurately this statement describes you.\n",
      "Options: (A). Very Accurate \n",
      "(B). Moderately Accurate \n",
      "(C). Neither Accurate Nor Inaccurate \n",
      "(D). Moderately Inaccurate \n",
      "(E). Very Inaccurate \n",
      "Answers: D\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/xiaoyangsong/Desktop/DSI Capstone/LLM-Personality-Codebase/Prompt/bert.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert.ipynb#ch0000008?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m (start, end) \u001b[39min\u001b[39;00m idx_lst:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert.ipynb#ch0000008?line=16'>17</a>\u001b[0m     mpi \u001b[39m=\u001b[39m MPI(local_path, start, end)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert.ipynb#ch0000008?line=17'>18</a>\u001b[0m     mpi\u001b[39m.\u001b[39;49mrun(tokenizer, model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert.ipynb#ch0000008?line=18'>19</a>\u001b[0m     mpi\u001b[39m.\u001b[39mdisplay_score()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert.ipynb#ch0000008?line=19'>20</a>\u001b[0m     MPI_OBJ_LST\u001b[39m.\u001b[39mappend(mpi)\n",
      "File \u001b[0;32m~/Desktop/DSI Capstone/LLM-Personality-Codebase/Prompt/bert_mpi.py:78\u001b[0m, in \u001b[0;36mMPI.run\u001b[0;34m(self, tokenizer, model)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert_mpi.py?line=74'>75</a>\u001b[0m \u001b[39mprint\u001b[39m(prompt \u001b[39m+\u001b[39m choice)\n\u001b[1;32m     <a href='file:///Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert_mpi.py?line=75'>76</a>\u001b[0m tokens \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     <a href='file:///Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert_mpi.py?line=76'>77</a>\u001b[0m     prompt \u001b[39m+\u001b[39m choice, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='file:///Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert_mpi.py?line=77'>78</a>\u001b[0m out \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokens)\n\u001b[1;32m     <a href='file:///Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert_mpi.py?line=78'>79</a>\u001b[0m logit \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     <a href='file:///Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert_mpi.py?line=79'>80</a>\u001b[0m \u001b[39m# Calculate Likelihood\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1226\u001b[0m, in \u001b[0;36mBertLMHeadModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1222'>1223</a>\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1223'>1224</a>\u001b[0m     use_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1225'>1226</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1226'>1227</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1227'>1228</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1228'>1229</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1229'>1230</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1230'>1231</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1231'>1232</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1232'>1233</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1233'>1234</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1234'>1235</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1235'>1236</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1236'>1237</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1237'>1238</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1238'>1239</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1239'>1240</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1241'>1242</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1242'>1243</a>\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=986'>987</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=988'>989</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=989'>990</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=990'>991</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=993'>994</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=994'>995</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=995'>996</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=996'>997</a>\u001b[0m     embedding_output,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=997'>998</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=998'>999</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=999'>1000</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1000'>1001</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1001'>1002</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1002'>1003</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1003'>1004</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1004'>1005</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1005'>1006</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1006'>1007</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1007'>1008</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1008'>1009</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=575'>576</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=576'>577</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=577'>578</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=581'>582</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=582'>583</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=583'>584</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=584'>585</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=585'>586</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=586'>587</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=587'>588</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=588'>589</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=589'>590</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=590'>591</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=591'>592</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=592'>593</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=594'>595</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=595'>596</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:513\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=509'>510</a>\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=510'>511</a>\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=512'>513</a>\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=513'>514</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=514'>515</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=515'>516</a>\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=517'>518</a>\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/modeling_utils.py:2928\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2924'>2925</a>\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2925'>2926</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2927'>2928</a>\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:525\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=523'>524</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=524'>525</a>\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=525'>526</a>\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=526'>527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:426\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=424'>425</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=425'>426</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=426'>427</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=427'>428</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=112'>113</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=113'>114</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bert_mpi import *\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-large-cased\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertLMHeadModel.from_pretrained(\"bert-base-uncased\")\n",
    "filename = \"mpi_small\"\n",
    "local_path = \"../Dataset/\" + f\"{filename}.csv\"\n",
    "\n",
    "MPI_OBJ_LST = []\n",
    "idx_lst = [(0, 120)]\n",
    "for (start, end) in idx_lst:\n",
    "    mpi = MPI(local_path, start, end)\n",
    "    mpi.run(tokenizer, model)\n",
    "    mpi.display_score()\n",
    "    MPI_OBJ_LST.append(mpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results (02/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 120 multiple choice questions in total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.questions.shape: (120,)\n",
      "ic| Counter(col): Counter({'N': 24, 'E': 24, 'O': 24, 'A': 24, 'C': 24})\n",
      "ic| len(Counter(col)): 5\n",
      "ic| Counter(col): Counter({1: 65, -1: 55})\n",
      "ic| len(Counter(col)): 2\n",
      "  0%|          | 0/120 [00:00<?, ?it/s]ic| pred.item(): 2\n",
      "  1%|          | 1/120 [00:02<03:59,  2.01s/it]ic| pred.item(): 2\n",
      "  2%|▏         | 2/120 [00:03<03:25,  1.74s/it]ic| pred.item(): 2\n",
      "  2%|▎         | 3/120 [00:05<03:17,  1.69s/it]ic| pred.item(): 2\n",
      "  3%|▎         | 4/120 [00:07<03:26,  1.78s/it]ic| pred.item(): 2\n",
      "  4%|▍         | 5/120 [00:09<03:40,  1.92s/it]ic| pred.item(): 2\n",
      "  5%|▌         | 6/120 [00:10<03:29,  1.83s/it]ic| pred.item(): 2\n",
      "  6%|▌         | 7/120 [00:12<03:29,  1.86s/it]ic| pred.item(): 2\n",
      "  7%|▋         | 8/120 [00:16<04:32,  2.43s/it]ic| pred.item(): 2\n",
      "  8%|▊         | 9/120 [00:20<05:37,  3.04s/it]ic| pred.item(): 2\n",
      "  8%|▊         | 10/120 [00:22<05:01,  2.74s/it]ic| pred.item(): 2\n",
      "  9%|▉         | 11/120 [00:24<04:26,  2.45s/it]ic| pred.item(): 2\n",
      " 10%|█         | 12/120 [00:27<04:40,  2.59s/it]ic| pred.item(): 2\n",
      " 11%|█         | 13/120 [00:30<04:36,  2.59s/it]ic| pred.item(): 2\n",
      " 12%|█▏        | 14/120 [00:32<04:31,  2.56s/it]ic| pred.item(): 2\n",
      " 12%|█▎        | 15/120 [00:34<03:56,  2.25s/it]ic| pred.item(): 2\n",
      " 13%|█▎        | 16/120 [00:36<03:47,  2.19s/it]ic| pred.item(): 2\n",
      " 14%|█▍        | 17/120 [00:37<03:27,  2.01s/it]ic| pred.item(): 2\n",
      " 15%|█▌        | 18/120 [00:39<03:13,  1.89s/it]ic| pred.item(): 2\n",
      " 16%|█▌        | 19/120 [00:41<03:11,  1.90s/it]ic| pred.item(): 2\n",
      " 17%|█▋        | 20/120 [00:43<03:01,  1.81s/it]ic| pred.item(): 2\n",
      " 18%|█▊        | 21/120 [00:45<03:04,  1.86s/it]ic| pred.item(): 2\n",
      " 18%|█▊        | 22/120 [00:46<03:04,  1.88s/it]ic| pred.item(): 2\n",
      " 19%|█▉        | 23/120 [00:48<03:05,  1.91s/it]ic| pred.item(): 2\n",
      " 20%|██        | 24/120 [00:50<02:57,  1.85s/it]ic| pred.item(): 2\n",
      " 21%|██        | 25/120 [00:53<03:15,  2.06s/it]ic| pred.item(): 2\n",
      " 22%|██▏       | 26/120 [00:55<03:25,  2.18s/it]ic| pred.item(): 2\n",
      " 22%|██▎       | 27/120 [00:57<03:06,  2.01s/it]ic| pred.item(): 3\n",
      " 23%|██▎       | 28/120 [01:00<03:26,  2.25s/it]ic| pred.item(): 2\n",
      " 24%|██▍       | 29/120 [01:03<04:03,  2.68s/it]ic| pred.item(): 2\n",
      " 25%|██▌       | 30/120 [01:06<04:07,  2.76s/it]ic| pred.item(): 2\n",
      " 26%|██▌       | 31/120 [01:08<03:39,  2.46s/it]ic| pred.item(): 2\n",
      " 27%|██▋       | 32/120 [01:10<03:16,  2.23s/it]ic| pred.item(): 2\n",
      " 28%|██▊       | 33/120 [01:12<03:18,  2.29s/it]ic| pred.item(): 2\n",
      " 28%|██▊       | 34/120 [01:14<03:13,  2.25s/it]ic| pred.item(): 2\n",
      " 29%|██▉       | 35/120 [01:18<03:37,  2.56s/it]ic| pred.item(): 2\n",
      " 30%|███       | 36/120 [01:21<03:50,  2.75s/it]ic| pred.item(): 2\n",
      " 31%|███       | 37/120 [01:24<04:00,  2.89s/it]ic| pred.item(): 2\n",
      " 32%|███▏      | 38/120 [01:27<04:06,  3.01s/it]ic| pred.item(): 2\n",
      " 32%|███▎      | 39/120 [01:30<04:05,  3.03s/it]ic| pred.item(): 2\n",
      " 33%|███▎      | 40/120 [01:34<04:19,  3.25s/it]ic| pred.item(): 2\n",
      " 34%|███▍      | 41/120 [01:37<04:08,  3.14s/it]ic| pred.item(): 2\n",
      " 35%|███▌      | 42/120 [01:40<04:10,  3.21s/it]ic| pred.item(): 2\n",
      " 36%|███▌      | 43/120 [01:46<04:56,  3.85s/it]ic| pred.item(): 2\n",
      " 37%|███▋      | 44/120 [01:48<04:27,  3.53s/it]ic| pred.item(): 2\n",
      " 38%|███▊      | 45/120 [01:50<03:39,  2.92s/it]ic| pred.item(): 2\n",
      " 38%|███▊      | 46/120 [01:52<03:07,  2.54s/it]ic| pred.item(): 2\n",
      " 39%|███▉      | 47/120 [01:53<02:48,  2.30s/it]ic| pred.item(): 2\n",
      " 40%|████      | 48/120 [01:56<03:00,  2.50s/it]ic| pred.item(): 2\n",
      " 41%|████      | 49/120 [01:59<02:55,  2.47s/it]ic| pred.item(): 2\n",
      " 42%|████▏     | 50/120 [02:03<03:37,  3.11s/it]ic| pred.item(): 2\n",
      " 42%|████▎     | 51/120 [02:07<03:45,  3.27s/it]ic| pred.item(): 2\n",
      " 43%|████▎     | 52/120 [02:10<03:44,  3.30s/it]ic| pred.item(): 2\n",
      " 44%|████▍     | 53/120 [02:13<03:22,  3.02s/it]ic| pred.item(): 2\n",
      " 45%|████▌     | 54/120 [02:15<03:04,  2.80s/it]ic| pred.item(): 2\n",
      " 46%|████▌     | 55/120 [02:17<02:51,  2.64s/it]ic| pred.item(): 2\n",
      " 47%|████▋     | 56/120 [02:20<02:41,  2.53s/it]ic| pred.item(): 2\n",
      " 48%|████▊     | 57/120 [02:22<02:44,  2.62s/it]ic| pred.item(): 2\n",
      " 48%|████▊     | 58/120 [02:26<02:56,  2.84s/it]ic| pred.item(): 2\n",
      " 49%|████▉     | 59/120 [02:29<03:03,  3.00s/it]ic| pred.item(): 2\n",
      " 50%|█████     | 60/120 [02:33<03:12,  3.20s/it]ic| pred.item(): 2\n",
      " 51%|█████     | 61/120 [02:36<03:05,  3.14s/it]ic| pred.item(): 2\n",
      " 52%|█████▏    | 62/120 [02:39<03:01,  3.13s/it]ic| pred.item(): 2\n",
      " 52%|█████▎    | 63/120 [02:42<02:55,  3.08s/it]ic| pred.item(): 2\n",
      " 53%|█████▎    | 64/120 [02:45<02:57,  3.17s/it]ic| pred.item(): 2\n",
      " 54%|█████▍    | 65/120 [02:49<03:00,  3.29s/it]ic| pred.item(): 2\n",
      " 55%|█████▌    | 66/120 [02:53<03:05,  3.43s/it]ic| pred.item(): 2\n",
      " 56%|█████▌    | 67/120 [02:56<03:02,  3.45s/it]ic| pred.item(): 2\n",
      " 57%|█████▋    | 68/120 [02:59<02:53,  3.34s/it]ic| pred.item(): 2\n",
      " 57%|█████▊    | 69/120 [03:03<02:55,  3.44s/it]ic| pred.item(): 2\n",
      " 58%|█████▊    | 70/120 [03:06<02:49,  3.40s/it]ic| pred.item(): 2\n",
      " 59%|█████▉    | 71/120 [03:10<02:50,  3.48s/it]ic| pred.item(): 2\n",
      " 60%|██████    | 72/120 [03:13<02:48,  3.52s/it]ic| pred.item(): 2\n",
      " 61%|██████    | 73/120 [03:16<02:32,  3.26s/it]ic| pred.item(): 2\n",
      " 62%|██████▏   | 74/120 [03:18<02:08,  2.80s/it]ic| pred.item(): 2\n",
      " 62%|██████▎   | 75/120 [03:19<01:49,  2.44s/it]ic| pred.item(): 2\n",
      " 63%|██████▎   | 76/120 [03:21<01:36,  2.20s/it]ic| pred.item(): 2\n",
      " 64%|██████▍   | 77/120 [03:23<01:27,  2.04s/it]ic| pred.item(): 2\n",
      " 65%|██████▌   | 78/120 [03:25<01:24,  2.01s/it]ic| pred.item(): 2\n",
      " 66%|██████▌   | 79/120 [03:34<02:49,  4.14s/it]ic| pred.item(): 2\n",
      " 67%|██████▋   | 80/120 [03:38<02:52,  4.32s/it]ic| pred.item(): 2\n",
      " 68%|██████▊   | 81/120 [03:40<02:17,  3.53s/it]ic| pred.item(): 2\n",
      " 68%|██████▊   | 82/120 [03:42<01:56,  3.06s/it]ic| pred.item(): 2\n",
      " 69%|██████▉   | 83/120 [03:45<01:48,  2.93s/it]ic| pred.item(): 2\n",
      " 70%|███████   | 84/120 [03:46<01:32,  2.57s/it]ic| pred.item(): 2\n",
      " 71%|███████   | 85/120 [03:48<01:20,  2.30s/it]ic| pred.item(): 2\n",
      " 72%|███████▏  | 86/120 [03:51<01:25,  2.52s/it]ic| pred.item(): 2\n",
      " 72%|███████▎  | 87/120 [03:55<01:33,  2.83s/it]ic| pred.item(): 3\n",
      " 73%|███████▎  | 88/120 [03:58<01:32,  2.89s/it]ic| pred.item(): 2\n",
      " 74%|███████▍  | 89/120 [04:00<01:19,  2.57s/it]ic| pred.item(): 2\n",
      " 75%|███████▌  | 90/120 [04:01<01:10,  2.34s/it]ic| pred.item(): 2\n",
      " 76%|███████▌  | 91/120 [04:03<01:02,  2.16s/it]ic| pred.item(): 2\n",
      " 77%|███████▋  | 92/120 [04:05<00:56,  2.01s/it]ic| pred.item(): 2\n",
      " 78%|███████▊  | 93/120 [04:07<00:54,  2.01s/it]ic| pred.item(): 2\n",
      " 78%|███████▊  | 94/120 [04:09<00:52,  2.03s/it]ic| pred.item(): 2\n",
      " 79%|███████▉  | 95/120 [04:13<01:08,  2.72s/it]ic| pred.item(): 2\n",
      " 80%|████████  | 96/120 [04:17<01:11,  2.98s/it]ic| pred.item(): 2\n",
      " 81%|████████  | 97/120 [04:20<01:10,  3.04s/it]ic| pred.item(): 2\n",
      " 82%|████████▏ | 98/120 [04:23<01:08,  3.12s/it]ic| pred.item(): 2\n",
      " 82%|████████▎ | 99/120 [04:27<01:07,  3.20s/it]ic| pred.item(): 2\n",
      " 83%|████████▎ | 100/120 [04:30<01:05,  3.29s/it]ic| pred.item(): 2\n",
      " 84%|████████▍ | 101/120 [04:34<01:05,  3.46s/it]ic| pred.item(): 2\n",
      " 85%|████████▌ | 102/120 [04:38<01:03,  3.52s/it]ic| pred.item(): 2\n",
      " 86%|████████▌ | 103/120 [04:41<01:01,  3.61s/it]ic| pred.item(): 2\n",
      " 87%|████████▋ | 104/120 [04:47<01:06,  4.14s/it]ic| pred.item(): 2\n",
      " 88%|████████▊ | 105/120 [04:50<00:57,  3.84s/it]ic| pred.item(): 2\n",
      " 88%|████████▊ | 106/120 [04:54<00:52,  3.78s/it]ic| pred.item(): 2\n",
      " 89%|████████▉ | 107/120 [04:57<00:46,  3.59s/it]ic| pred.item(): 2\n",
      " 90%|█████████ | 108/120 [05:00<00:42,  3.56s/it]ic| pred.item(): 2\n",
      " 91%|█████████ | 109/120 [05:04<00:40,  3.65s/it]ic| pred.item(): 2\n",
      " 92%|█████████▏| 110/120 [05:08<00:35,  3.58s/it]ic| pred.item(): 2\n",
      " 92%|█████████▎| 111/120 [05:11<00:32,  3.63s/it]ic| pred.item(): 2\n",
      " 93%|█████████▎| 112/120 [05:15<00:28,  3.53s/it]ic| pred.item(): 2\n",
      " 94%|█████████▍| 113/120 [05:18<00:24,  3.44s/it]ic| pred.item(): 2\n",
      " 95%|█████████▌| 114/120 [05:22<00:21,  3.64s/it]ic| pred.item(): 2\n",
      " 96%|█████████▌| 115/120 [05:25<00:17,  3.50s/it]ic| pred.item(): 2\n",
      " 97%|█████████▋| 116/120 [05:28<00:13,  3.36s/it]ic| pred.item(): 2\n",
      " 98%|█████████▊| 117/120 [05:32<00:10,  3.55s/it]ic| pred.item(): 2\n",
      " 98%|█████████▊| 118/120 [05:36<00:07,  3.57s/it]ic| pred.item(): 2\n",
      " 99%|█████████▉| 119/120 [05:39<00:03,  3.58s/it]ic| pred.item(): 2\n",
      "100%|██████████| 120/120 [05:43<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N | MEAN: 1.25 | STD: 2.7858338356018066\n",
      "E | MEAN: 1.5 | STD: 2.6539552211761475\n",
      "O | MEAN: 0.0 | STD: 2.9927449226379395\n",
      "A | MEAN: -1.25 | STD: 2.7858338356018066\n",
      "C | MEAN: -0.25 | STD: 3.0538642406463623\n"
     ]
    }
   ],
   "source": [
    "from bert_mpi import *\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-large-cased\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertLMHeadModel.from_pretrained(\"bert-base-uncased\")\n",
    "filename = \"mpi_small\"\n",
    "local_path = \"../Dataset/\" + f\"{filename}.csv\"\n",
    "\n",
    "MPI_OBJ_LST = []\n",
    "idx_lst = [(0, 120)]\n",
    "for (start, end) in idx_lst:\n",
    "    mpi = MPI(local_path, start, end)\n",
    "    mpi.run(tokenizer, model)\n",
    "    mpi.display_score()\n",
    "    MPI_OBJ_LST.append(mpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 64, -3: 54, 2: 1, -2: 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(np.array(mpi.scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N | MEAN: 1.25 | STD: 2.7858338356018066\n",
    "\n",
    "E | MEAN: 1.5 | STD: 2.6539552211761475\n",
    "\n",
    "O | MEAN: 0.0 | STD: 2.9927449226379395\n",
    "\n",
    "A | MEAN: -1.25 | STD: 2.7858338356018066\n",
    "\n",
    "C | MEAN: -0.25 | STD: 3.0538642406463623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
