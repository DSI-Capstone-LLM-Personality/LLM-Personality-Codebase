{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- #\n",
    "# NOTEBOOK MPI EXPERIMENT #\n",
    "# AUTHOR: XIAOYANG SONG   #\n",
    "# ----------------------- #\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tabulate import tabulate\n",
    "sys.path.append('../')\n",
    "from bert_mpi import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from bert_mpi import *\n",
    "from transformers import AutoTokenizer, BertForMultipleChoice, BertLMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertLMHeadModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| prob.shape: torch.Size([67, 30522])\n",
      "ic| torch.sum(torch.log(logits)): tensor(-6.6448, grad_fn=<SumBackward0>)\n",
      "ic| logits: tensor([0.0278, 0.2255, 0.9994, 0.9981, 0.9998, 0.9954, 0.9995, 1.0000, 0.9994,\n",
      "                    0.9979, 0.9877, 0.9978, 1.0000, 1.0000, 0.9717, 0.8981, 0.8578, 0.9991,\n",
      "                    0.9992, 0.9904, 0.9999, 0.8551, 1.0000, 0.9916, 0.9933, 0.9986, 0.9736,\n",
      "                    0.9981, 1.0000, 0.9947, 1.0000, 0.9998, 0.9999, 1.0000, 1.0000, 0.9971,\n",
      "                    0.9782, 1.0000, 0.9990, 1.0000, 0.9999, 0.9404, 0.9561, 1.0000, 0.9912,\n",
      "                    0.9978, 1.0000, 0.9983, 0.9956, 0.9996, 0.6608, 1.0000, 0.9995, 1.0000,\n",
      "                    1.0000, 0.9520, 0.7708, 1.0000, 0.9990, 0.9816, 1.0000, 0.9985, 0.9171,\n",
      "                    0.9871, 1.0000, 0.9685, 0.9895], grad_fn=<MaxBackward0>)\n",
      "ic| logits.shape: torch.Size([67])\n",
      "ic| loss: None\n"
     ]
    }
   ],
   "source": [
    "t1 = \"Make friends easily\"\n",
    "t2 = \"Trust other people\"\n",
    "t3 = \"Have difficulty imagining things\"\n",
    "t_lst = [t1, t2, t3]\n",
    "prompt_lst = [prepare_mpi_questions(t) for t in t_lst]\n",
    "# ic(prompt_lst[0])\n",
    "inputs = tokenizer(prompt_lst[0] +  \"A\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "outputs = model(**inputs)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "# choice0 = \"It is eaten with a fork and a knife.\"\n",
    "# choice1 = \"It is eaten while held in the hand.\"\n",
    "# labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "# statement = \"Trust others.\"\n",
    "t1 = \"Make friends easily\"\n",
    "t2 = \"Trust other people\"\n",
    "t3 = \"Have difficulty imagining things\"\n",
    "t_lst = [t1, t2, t3]\n",
    "prompt_lst = [prepare_mpi_questions(t) for t in t_lst]\n",
    "\n",
    "\n",
    "# prompt_lst = []\n",
    "for prompt in prompt_lst:\n",
    "    encoding = tokenizer([prompt]*5, ['A', 'B', 'C', 'D', 'E'], return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()})  # batch size is 1\n",
    "\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    ic(logits)\n",
    "    print(MPI_IDX_TO_KEY[torch.argmax(logits.squeeze()).item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from bert_mpi import *\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-large-cased\")\n",
    "version = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(version)\n",
    "model = BertLMHeadModel.from_pretrained(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET LOADING\n",
    "from bert_mpi import *\n",
    "filename = \"mpi_small\"\n",
    "local_path = \"../Dataset/\" + f\"{filename}.csv\"\n",
    "start, end = 0, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of answers & whole sentence answer (i.e. all)\n",
    "log = \"mpi_all_mean_a\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICE_ALL, 'mean-a', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [03:45<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of answers & naive answer\n",
    "log = \"mpi_naive_mean_a\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICES, 'mean-a', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [03:20<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of answers & description only\n",
    "log = \"mpi_desc_mean_a\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICES_DESC, 'mean-a', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [03:50<00:00,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of sentences & whole sentence answer (i.e. all)\n",
    "log = \"mpi_all_mean_s\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICE_ALL, 'mean-s', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [04:01<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of sentences & naive answer\n",
    "log = \"mpi_naive_mean_s\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICES, 'mean-s', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean-ll of sentences & description only\n",
    "log = \"mpi_desc_mean_s\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICES_DESC, 'mean-s', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
