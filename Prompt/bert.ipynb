{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tabulate import tabulate\n",
    "sys.path.append('../')\n",
    "from bert_mpi import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from bert_mpi import *\n",
    "from transformers import AutoTokenizer, BertForMultipleChoice, BertLMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertLMHeadModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| prob.shape: torch.Size([67, 30522])\n",
      "ic| torch.sum(torch.log(logits)): tensor(-6.6448, grad_fn=<SumBackward0>)\n",
      "ic| logits: tensor([0.0278, 0.2255, 0.9994, 0.9981, 0.9998, 0.9954, 0.9995, 1.0000, 0.9994,\n",
      "                    0.9979, 0.9877, 0.9978, 1.0000, 1.0000, 0.9717, 0.8981, 0.8578, 0.9991,\n",
      "                    0.9992, 0.9904, 0.9999, 0.8551, 1.0000, 0.9916, 0.9933, 0.9986, 0.9736,\n",
      "                    0.9981, 1.0000, 0.9947, 1.0000, 0.9998, 0.9999, 1.0000, 1.0000, 0.9971,\n",
      "                    0.9782, 1.0000, 0.9990, 1.0000, 0.9999, 0.9404, 0.9561, 1.0000, 0.9912,\n",
      "                    0.9978, 1.0000, 0.9983, 0.9956, 0.9996, 0.6608, 1.0000, 0.9995, 1.0000,\n",
      "                    1.0000, 0.9520, 0.7708, 1.0000, 0.9990, 0.9816, 1.0000, 0.9985, 0.9171,\n",
      "                    0.9871, 1.0000, 0.9685, 0.9895], grad_fn=<MaxBackward0>)\n",
      "ic| logits.shape: torch.Size([67])\n",
      "ic| loss: None\n"
     ]
    }
   ],
   "source": [
    "t1 = \"Make friends easily\"\n",
    "t2 = \"Trust other people\"\n",
    "t3 = \"Have difficulty imagining things\"\n",
    "t_lst = [t1, t2, t3]\n",
    "prompt_lst = [prepare_mpi_questions(t) for t in t_lst]\n",
    "# ic(prompt_lst[0])\n",
    "inputs = tokenizer(prompt_lst[0] +  \"A\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "outputs = model(**inputs)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "# choice0 = \"It is eaten with a fork and a knife.\"\n",
    "# choice1 = \"It is eaten while held in the hand.\"\n",
    "# labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "# statement = \"Trust others.\"\n",
    "t1 = \"Make friends easily\"\n",
    "t2 = \"Trust other people\"\n",
    "t3 = \"Have difficulty imagining things\"\n",
    "t_lst = [t1, t2, t3]\n",
    "prompt_lst = [prepare_mpi_questions(t) for t in t_lst]\n",
    "\n",
    "\n",
    "# prompt_lst = []\n",
    "for prompt in prompt_lst:\n",
    "    encoding = tokenizer([prompt]*5, ['A', 'B', 'C', 'D', 'E'], return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()})  # batch size is 1\n",
    "\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    ic(logits)\n",
    "    print(MPI_IDX_TO_KEY[torch.argmax(logits.squeeze()).item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from bert_mpi import *\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-large-cased\")\n",
    "version = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(version)\n",
    "model = BertLMHeadModel.from_pretrained(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET LOADING\n",
    "from bert_mpi import *\n",
    "filename = \"mpi_small\"\n",
    "local_path = \"../Dataset/\" + f\"{filename}.csv\"\n",
    "start, end = 0, 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [04:09<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of answers & whole sentence answer (i.e. all)\n",
    "log = \"mpi_all_mean_a\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICE_ALL, 'mean-a', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [03:53<00:00,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of answers & naive answer\n",
    "log = \"mpi_naive_mean_a\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICES, 'mean-a', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [04:09<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of answers & description only\n",
    "log = \"mpi_desc_mean_a\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICES_DESC, 'mean-a', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = \"mpi_desc_mean_a\"\n",
    "mpi = torch.load(f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(Counter(np.array(mpi.OCEAN['E'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+-----+-----+-----+-----+-----+\n",
      "|   -5 |   -4 |   -3 |   -2 |   -1 |   1 |   2 |   3 |   4 |   5 |\n",
      "|------+------+------+------+------+-----+-----+-----+-----+-----|\n",
      "|   -5 |   -4 |   -3 |   -2 |   -1 |   1 |   2 |   3 |   4 |   5 |\n",
      "+------+------+------+------+------+-----+-----+-----+-----+-----+\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[[-5,-4,-3,-2,-1,1,2,3,4,5]] = None\n",
    "df.loc[len(df.index)] = [-5,-4,-3,-2,-1,1,2,3,4,5]\n",
    "print(tabulate(df, headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -5  -4  -3  -2  -1   1   2   3   4   5\n",
      "0  -5  -4  -3  -2  -1   1   2   3   4   5\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[[-5,-4,-3,-2,-1,1,2,3,4,5]] = None\n",
    "df.loc[len(df.index)] = [-5,-4,-3,-2,-1,1,2,3,4,5]\n",
    "print(df)\n",
    "with open(\"text.txt\", 'w') as f:\n",
    "    ori = sys.stdout\n",
    "    sys.stdout = f\n",
    "    print(tabulate(df, headers='keys', tablefmt='psql', showindex=False))\n",
    "    sys.stdout = ori\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = list(np.arange(-5,0,1)) + list(np.arange(1,6,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(score=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0 -5\n",
       "1 -4\n",
       "2 -3\n",
       "3 -2\n",
       "4 -1\n",
       "5  1\n",
       "6  2\n",
       "7  3\n",
       "8  4\n",
       "9  5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5  | 0\n",
      "-4  | 0\n",
      "-3  | 1\n",
      "-2  | 3\n",
      "-1  | 2\n",
      "1   | 8\n",
      "2   | 5\n",
      "3   | 4\n",
      "4   | 0\n",
      "5   | 1\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "val_lst = []\n",
    "for item in [-5,-4,-3,-2,-1,1,2,3,4,5]:\n",
    "    val = d[item] if item in d else 0\n",
    "    val_lst.append(val)\n",
    "    df[item] = val\n",
    "    print(f\"{item:<3} | {val}\")\n",
    "df.loc[len(df.index)] = val_lst "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   -5 |   -4 |   -3 |   -2 |   -1 |   1 |   2 |   3 |   4 |   5 |\n",
    "|-----:|-----:|-----:|-----:|-----:|----:|----:|----:|----:|----:|\n",
    "|    0 |    0 |    4 |    0 |    8 |   4 |   1 |   6 |   0 |   1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   -5 |   -4 |   -3 |   -2 |   -1 |   1 |   2 |   3 |   4 |   5 |\n",
      "|-----:|-----:|-----:|-----:|-----:|----:|----:|----:|----:|----:|\n",
      "|    0 |    0 |    1 |    3 |    2 |   8 |   5 |   4 |   0 |   1 |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "# print(df)\n",
    "print(df.to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
