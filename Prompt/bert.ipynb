{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- #\n",
    "# NOTEBOOK MPI EXPERIMENT #\n",
    "# AUTHOR: XIAOYANG SONG   #\n",
    "# ----------------------- #\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tabulate import tabulate\n",
    "sys.path.append('../')\n",
    "from mpi import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from bert_mpi import *\n",
    "from transformers import AutoTokenizer, BertForMultipleChoice, BertLMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertLMHeadModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = \"Make friends easily\"\n",
    "t2 = \"Trust other people\"\n",
    "t3 = \"Have difficulty imagining things\"\n",
    "t_lst = [t1, t2, t3]\n",
    "prompt_lst = [prepare_mpi_questions(t) for t in t_lst]\n",
    "# ic(prompt_lst[0])\n",
    "inputs = tokenizer(prompt_lst[0] +  \"A\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "outputs = model(**inputs)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from mpi import *\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-large-cased\")\n",
    "version = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(version)\n",
    "model = BertLMHeadModel.from_pretrained(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertLMHeadModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# DATASET LOADING\n",
    "from bert_mpi import *\n",
    "filename = \"mpi_small\"\n",
    "local_path = \"../Dataset/\" + f\"{filename}.csv\"\n",
    "start, end = 0, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_config=dict(path_to_dset = local_path, start_idx=0, end_idx=10)\n",
    "model_config = dict(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    desc={'family': 'BERT', 'version': version}\n",
    ")\n",
    "algo_config=dict(ll_type='mean-a')\n",
    "template_config=dict(\n",
    "    prompt = MPI_PROMPT,\n",
    "    option = MPI_CHOICE_ALL,\n",
    "    choice= MPI_CHOICES_DESC\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Sample questions look like this:\n",
      "Given a statement of you: \"You Worry about things.\" Please choose from the following options to identify how accurately this statement describes you.\n",
      "Options: \n",
      "(A). Very Accurate \n",
      "(B). Moderately Accurate \n",
      "(C). Neither Accurate Nor Inaccurate \n",
      "(D). Moderately Inaccurate \n",
      "(E). Very Inaccurate \n",
      "Answers: \n",
      "----------------------------------------\n",
      "MCQA task starts...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:11,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION #0    | TRAIT: N | KEY: + | SCORE: 1 | ANSWER: Very Inaccurate\n",
      "-- Inverse Log-Perplexity: [-0.0725, -0.1208, -0.0843, -0.0954, -0.0608]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:10,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION #1    | TRAIT: E | KEY: + | SCORE: 3 | ANSWER: Neither Accurate Nor Inaccurate\n",
      "-- Inverse Log-Perplexity: [-0.1042, -0.095, -0.0724, -0.08, -0.0824]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:03<00:08,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION #2    | TRAIT: O | KEY: + | SCORE: 3 | ANSWER: Neither Accurate Nor Inaccurate\n",
      "-- Inverse Log-Perplexity: [-0.0921, -0.0851, -0.0658, -0.0703, -0.0806]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:05<00:07,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION #3    | TRAIT: A | KEY: + | SCORE: 1 | ANSWER: Very Inaccurate\n",
      "-- Inverse Log-Perplexity: [-0.0731, -0.0806, -0.0737, -0.0668, -0.0529]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:06<00:06,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION #4    | TRAIT: C | KEY: + | SCORE: 3 | ANSWER: Neither Accurate Nor Inaccurate\n",
      "-- Inverse Log-Perplexity: [-0.1153, -0.1224, -0.0834, -0.1006, -0.0974]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:07<00:04,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION #5    | TRAIT: N | KEY: + | SCORE: 3 | ANSWER: Neither Accurate Nor Inaccurate\n",
      "-- Inverse Log-Perplexity: [-0.1192, -0.1026, -0.0736, -0.0873, -0.091]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:08<00:03,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION #6    | TRAIT: E | KEY: + | SCORE: 3 | ANSWER: Neither Accurate Nor Inaccurate\n",
      "-- Inverse Log-Perplexity: [-0.0903, -0.0982, -0.0658, -0.0845, -0.0814]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:10<00:02,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION #7    | TRAIT: O | KEY: + | SCORE: 3 | ANSWER: Neither Accurate Nor Inaccurate\n",
      "-- Inverse Log-Perplexity: [-0.0779, -0.0864, -0.0625, -0.07, -0.0706]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:11<00:01,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION #8    | TRAIT: A | KEY: - | SCORE: 5 | ANSWER: Very Inaccurate\n",
      "-- Inverse Log-Perplexity: [-0.0781, -0.092, -0.0704, -0.0737, -0.0684]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION #9    | TRAIT: C | KEY: + | SCORE: 1 | ANSWER: Very Inaccurate\n",
      "-- Inverse Log-Perplexity: [-0.0792, -0.1174, -0.082, -0.0922, -0.0548]\n",
      "----------------------------------------\n",
      "OCEAN SCORES STATS\n",
      "O | MEAN: 3.0      | STD: 0.0\n",
      "C | MEAN: 2.0      | STD: 1.41421\n",
      "E | MEAN: 3.0      | STD: 0.0\n",
      "A | MEAN: 3.0      | STD: 2.82843\n",
      "N | MEAN: 2.0      | STD: 1.41421\n",
      "----------------------------------------\n",
      "OTHER INTERESTING STATS\n",
      "ANSWERS                         | Count\n",
      "Very Accurate                   |   0\n",
      "Moderately Accurate             |   0\n",
      "Neither Accurate Nor Inaccurate |   6\n",
      "Moderately Inaccurate           |   0\n",
      "Very Inaccurate                 |   4\n",
      "----------------------------------------\n",
      "TRAITS-LEVEL STATS: \n",
      "Trait: O | # Questions: 2\n",
      "> CHOICES DISTRIBUTION [+]\n",
      "ANSWERS                         | Count\n",
      "Very Accurate                   |   0\n",
      "Moderately Accurate             |   0\n",
      "Neither Accurate Nor Inaccurate |   2\n",
      "Moderately Inaccurate           |   0\n",
      "Very Inaccurate                 |   0\n",
      "\n",
      "> CHOICES DISTRIBUTION [-]\n",
      "ANSWERS                         | Count\n",
      "Very Accurate                   |   0\n",
      "Moderately Accurate             |   0\n",
      "Neither Accurate Nor Inaccurate |   0\n",
      "Moderately Inaccurate           |   0\n",
      "Very Inaccurate                 |   0\n",
      "\n",
      "> SCORE DISTRIBUTION\n",
      "+-----+-----+-----+-----+-----+\n",
      "|   1 |   2 |   3 |   4 |   5 |\n",
      "|-----+-----+-----+-----+-----|\n",
      "|   0 |   0 |   2 |   0 |   0 |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "Trait: C | # Questions: 2\n",
      "> CHOICES DISTRIBUTION [+]\n",
      "ANSWERS                         | Count\n",
      "Very Accurate                   |   0\n",
      "Moderately Accurate             |   0\n",
      "Neither Accurate Nor Inaccurate |   1\n",
      "Moderately Inaccurate           |   0\n",
      "Very Inaccurate                 |   1\n",
      "\n",
      "> CHOICES DISTRIBUTION [-]\n",
      "ANSWERS                         | Count\n",
      "Very Accurate                   |   0\n",
      "Moderately Accurate             |   0\n",
      "Neither Accurate Nor Inaccurate |   0\n",
      "Moderately Inaccurate           |   0\n",
      "Very Inaccurate                 |   0\n",
      "\n",
      "> SCORE DISTRIBUTION\n",
      "+-----+-----+-----+-----+-----+\n",
      "|   1 |   2 |   3 |   4 |   5 |\n",
      "|-----+-----+-----+-----+-----|\n",
      "|   1 |   0 |   1 |   0 |   0 |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "Trait: E | # Questions: 2\n",
      "> CHOICES DISTRIBUTION [+]\n",
      "ANSWERS                         | Count\n",
      "Very Accurate                   |   0\n",
      "Moderately Accurate             |   0\n",
      "Neither Accurate Nor Inaccurate |   2\n",
      "Moderately Inaccurate           |   0\n",
      "Very Inaccurate                 |   0\n",
      "\n",
      "> CHOICES DISTRIBUTION [-]\n",
      "ANSWERS                         | Count\n",
      "Very Accurate                   |   0\n",
      "Moderately Accurate             |   0\n",
      "Neither Accurate Nor Inaccurate |   0\n",
      "Moderately Inaccurate           |   0\n",
      "Very Inaccurate                 |   0\n",
      "\n",
      "> SCORE DISTRIBUTION\n",
      "+-----+-----+-----+-----+-----+\n",
      "|   1 |   2 |   3 |   4 |   5 |\n",
      "|-----+-----+-----+-----+-----|\n",
      "|   0 |   0 |   2 |   0 |   0 |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "Trait: A | # Questions: 2\n",
      "> CHOICES DISTRIBUTION [+]\n",
      "ANSWERS                         | Count\n",
      "Very Accurate                   |   0\n",
      "Moderately Accurate             |   0\n",
      "Neither Accurate Nor Inaccurate |   0\n",
      "Moderately Inaccurate           |   0\n",
      "Very Inaccurate                 |   1\n",
      "\n",
      "> CHOICES DISTRIBUTION [-]\n",
      "ANSWERS                         | Count\n",
      "Very Accurate                   |   0\n",
      "Moderately Accurate             |   0\n",
      "Neither Accurate Nor Inaccurate |   0\n",
      "Moderately Inaccurate           |   0\n",
      "Very Inaccurate                 |   1\n",
      "\n",
      "> SCORE DISTRIBUTION\n",
      "+-----+-----+-----+-----+-----+\n",
      "|   1 |   2 |   3 |   4 |   5 |\n",
      "|-----+-----+-----+-----+-----|\n",
      "|   1 |   0 |   0 |   0 |   1 |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "Trait: N | # Questions: 2\n",
      "> CHOICES DISTRIBUTION [+]\n",
      "ANSWERS                         | Count\n",
      "Very Accurate                   |   0\n",
      "Moderately Accurate             |   0\n",
      "Neither Accurate Nor Inaccurate |   1\n",
      "Moderately Inaccurate           |   0\n",
      "Very Inaccurate                 |   1\n",
      "\n",
      "> CHOICES DISTRIBUTION [-]\n",
      "ANSWERS                         | Count\n",
      "Very Accurate                   |   0\n",
      "Moderately Accurate             |   0\n",
      "Neither Accurate Nor Inaccurate |   0\n",
      "Moderately Inaccurate           |   0\n",
      "Very Inaccurate                 |   0\n",
      "\n",
      "> SCORE DISTRIBUTION\n",
      "+-----+-----+-----+-----+-----+\n",
      "|   1 |   2 |   3 |   4 |   5 |\n",
      "|-----+-----+-----+-----+-----|\n",
      "|   1 |   0 |   1 |   0 |   0 |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "log = \"trial\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(dset_config, model_config, algo_config, template_config, filename, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [03:35<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of answers & whole sentence answer (i.e. all)\n",
    "log = \"mpi_all_mean_a\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICE_ALL, 'mean-a', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [03:38<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of answers & naive answer\n",
    "log = \"mpi_naive_mean_a\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICES, 'mean-a', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [03:44<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of answers & description only\n",
    "log = \"mpi_desc_mean_a\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICES_DESC, 'mean-a', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [03:29<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of sentences & whole sentence answer (i.e. all)\n",
    "log = \"mpi_all_mean_s\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICE_ALL, 'mean-s', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [03:16<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of sentences & naive answer\n",
    "log = \"mpi_naive_mean_s\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICES, 'mean-s', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [03:27<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of sentences & description only\n",
    "log = \"mpi_desc_mean_s\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICES_DESC, 'mean-s', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Template reordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [02:37<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean-ll of answers & description only\n",
    "log = \"mpi_desc_mean_a_[reordered_template][ACDEB]\"\n",
    "filename = f\"../checkpoint/log/{log}.txt\"\n",
    "mpi = run_mpi(local_path, start, end, model, tokenizer, version, MPI_CHOICES_DESC, 'mean-a', filename)\n",
    "torch.save(mpi, f\"../checkpoint/mpis/{log}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| outputs.last_hidden_state.shape: torch.Size([1, 8, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForCausalLM, AutoConfig\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "config = AutoConfig.from_pretrained(\"roberta-base\")\n",
    "config.is_decoder = True\n",
    "model = RobertaForCausalLM.from_pretrained(\"roberta-base\", config=config)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "prediction_logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 50265])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_logits.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
