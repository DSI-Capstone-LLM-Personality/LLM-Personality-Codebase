{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- #\n",
    "# NOTEBOOK MPI EXPERIMENT #\n",
    "# AUTHOR: XIAOYANG SONG   #\n",
    "# ----------------------- #\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tabulate import tabulate\n",
    "sys.path.append('../')\n",
    "from mpi import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from mpi import *\n",
    "\n",
    "dir_path = \"../Dataset/\"\n",
    "log_dir, ckpt_dir = \"../checkpoint/log/\", \"../checkpoint/mpis/\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-large-cased\")\n",
    "# version = \"bert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(version)\n",
    "# model = BertLMHeadModel.from_pretrained(version)\n",
    "# model_config = dict(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     desc={'family': 'BERT', 'version': version}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dset_lst = ['ocean_15']\n",
    "# ALGO\n",
    "algo_config = {'ll_type':'ans_inv_perp'}\n",
    "# MODEL\n",
    "version = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(version)\n",
    "model = BertLMHeadModel.from_pretrained(version)\n",
    "model_config = dict(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    desc={'family': 'BERT', 'version': version}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_lst = ['choice-only', 'desc-only', 'choice-desc']\n",
    "for dset in dset_lst:\n",
    "    path = dir_path + f\"{dset}.csv\"\n",
    "    dset_config=dict(path_to_dset=path, start_idx=None, end_idx=None)\n",
    "    for choice in choice_lst:\n",
    "        template_config=dict(prompt = MPI_PROMPT,option=CHOICE[choice],choice=CHOICE[choice],shuffle=False)\n",
    "        filename = log_fname(dset, model_config['desc'], choice, algo_config['ll_type'])\n",
    "        print(filename)\n",
    "        mpi = run_mpi(dset_config, model_config, algo_config, template_config, log_dir+f'{filename}.txt', False)\n",
    "        torch.save(mpi, ckpt_dir+f\"{filename}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT-base shuffling order MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"../Dataset/\"\n",
    "log_dir, ckpt_dir = \"../checkpoint/log/BERT-shuffle/\", \"../checkpoint/mpis/BERT-shuffle/\"\n",
    "dset_lst = ['ocean_15']\n",
    "# ALGO\n",
    "algo_config = {'ll_type':'ans_inv_perp'}\n",
    "# MODEL\n",
    "version = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(version)\n",
    "model = BertLMHeadModel.from_pretrained(version)\n",
    "model_config = dict(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    desc={'family': 'BERT', 'version': version}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:16<00:00,  1.12s/it]\n",
      "100%|██████████| 15/15 [00:16<00:00,  1.13s/it]\n",
      "100%|██████████| 15/15 [00:16<00:00,  1.13s/it]\n",
      "100%|██████████| 15/15 [00:16<00:00,  1.12s/it]\n",
      "100%|██████████| 15/15 [00:16<00:00,  1.12s/it]\n",
      "100%|██████████| 15/15 [00:14<00:00,  1.06it/s]\n",
      "100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n",
      "100%|██████████| 15/15 [00:14<00:00,  1.04it/s]\n",
      "100%|██████████| 15/15 [00:14<00:00,  1.03it/s]\n",
      "100%|██████████| 15/15 [00:16<00:00,  1.07s/it]\n",
      "100%|██████████| 15/15 [00:20<00:00,  1.35s/it]\n",
      "100%|██████████| 15/15 [00:20<00:00,  1.35s/it]\n",
      "100%|██████████| 15/15 [00:19<00:00,  1.30s/it]\n",
      "100%|██████████| 15/15 [00:19<00:00,  1.30s/it]\n",
      "100%|██████████| 15/15 [00:19<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "choice_lst = ['choice-only', 'desc-only', 'choice-desc']\n",
    "num_mc = 5\n",
    "for dset in dset_lst:\n",
    "    path = dir_path + f\"{dset}.csv\"\n",
    "    dset_config=dict(path_to_dset=path, start_idx=None, end_idx=None)\n",
    "    for choice in choice_lst:\n",
    "        # SHUFFLE: ORDER SYMMETRY\n",
    "        for idx in range(num_mc):\n",
    "            option = {'+':CHOICE[choice], '-':CHOICE[choice]}\n",
    "            mpi_choice = {'+':CHOICE[choice], '-':CHOICE[choice]}\n",
    "            template_config=dict(prompt = MPI_PROMPT,option=option,choice=mpi_choice,shuffle=True)\n",
    "            filename = log_fname(dset, model_config['desc'], choice, algo_config['ll_type'])\n",
    "            filename = filename + f\"_[mc{idx}]\"\n",
    "            mpi = run_mpi(dset_config, model_config, algo_config, template_config, log_dir+f'{filename}.txt', False)\n",
    "            torch.save(mpi, ckpt_dir+f\"{filename}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| outputs.last_hidden_state.shape: torch.Size([1, 8, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForCausalLM, AutoConfig\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "config = AutoConfig.from_pretrained(\"roberta-base\")\n",
    "config.is_decoder = True\n",
    "model = RobertaForCausalLM.from_pretrained(\"roberta-base\", config=config)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "prediction_logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 50265])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_logits.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
