{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip install transformers\n",
    "# !pip install icecream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d885e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model init\n",
      "Choice 1:\n",
      "-1087.692138671875\n",
      "-1136.8531494140625\n",
      "-1376.4542236328125\n",
      "-1219.9056396484375\n",
      "-1174.92333984375\n",
      "\n",
      "Choice 2:\n",
      "-617.5750732421875\n",
      "-595.3192749023438\n",
      "-609.1790161132812\n",
      "-612.5242309570312\n",
      "-617.8251953125\n"
     ]
    }
   ],
   "source": [
    "# Compute sentence probability\n",
    "# https://huggingface.co/docs/transformers/model_doc/gpt2\n",
    "\n",
    "from icecream import ic\n",
    "import torch\n",
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    " \n",
    "def model_init(model_string, cuda):\n",
    "    if model_string.startswith(\"gpt2\"):\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_string)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_string)\n",
    "    else:\n",
    "        tokenizer = OpenAIGPTTokenizer.from_pretrained(model_string)\n",
    "        model = OpenAIGPTLMHeadModel.from_pretrained(model_string)\n",
    "    model.eval()\n",
    "    if cuda:\n",
    "        model.to('cuda')\n",
    "    print(\"Model init\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def sent_scoring(model_tokenizer, text, cuda):\n",
    "    model = model_tokenizer[0]\n",
    "    tokenizer = model_tokenizer[1]\n",
    "    assert model is not None\n",
    "    assert tokenizer is not None\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    if cuda:\n",
    "        input_ids = input_ids.to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        \n",
    "    loss, logits = outputs[:2] \n",
    "    loss = loss.item() #loss - next tokenizer prediction (output)\n",
    "#     ic(loss)#4.990908145904541  5.691289901733398\n",
    "\n",
    "#logits- scores for each vocabulary token before SoftMax ,maps probabilities [0, 1] to [-inf, +inf]\n",
    "#logits.shape: torch.Size([1 batch size, 7 input size, 50257 vocab size])\n",
    "#     ic(logits)\n",
    "    word_prob = torch.max(logits.squeeze(), dim=-1)[0] #max prob for each tokenizer in the input\n",
    "#     print(word_prob)\n",
    "    sentence_prob = torch.sum(word_prob).item()\n",
    "    return sentence_prob\n",
    "#     return sentence_prob\n",
    " \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # model, tokenizer = model_init('openai-gpt', False) \n",
    "    model, tokenizer = model_init('gpt2', False) \n",
    "    choice1 = [\"Very Accurate\",\"Moderately Accurate\", \"Neither Accurate Nor Inaccurate\", \"Moderately Inaccurate\",\"Very Inaccurate\"]\n",
    "    choice2 = [\"always\", \"often\",\"sometimes\",\"rarely\",\"never\"]\n",
    "    \n",
    "    print(\"Choice 1:\")\n",
    "    for i in range(len(choice1)):\n",
    "        print(sent_scoring((model, tokenizer), f\"I have difficulty imagining things. Answer: {choice1[i]}\", False))\n",
    "    print(\"\\nChoice 2:\")\n",
    "    for i in range(len(choice2)):\n",
    "        print(sent_scoring((model, tokenizer), f\"I {choice2[i]} have difficulty imagining things\", False))\n",
    "\n",
    "# in-sentence:    \n",
    "# -617.5750732421875 \n",
    "# -617.8251953125 \n",
    "\n",
    "# as answer: choice2\n",
    "# -935.1431884765625\n",
    "# -934.5279541015625\n",
    "\n",
    "# as answer: choice1\n",
    "# -1087.692138671875\n",
    "# -1174.92333984375\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5ad0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tree/opt/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/tree/opt/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175/15\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "num_return_sequences = 3\n",
    "\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_ids = tokenizer(\"Today is a nice day\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "generated_outputs = gpt2.generate(input_ids, do_sample=True, \n",
    "                                  num_return_sequences=num_return_sequences, \n",
    "                                  output_scores=True)\n",
    "\n",
    "# only use id's that were generated\n",
    "# gen_sequences has shape [3, 15]\n",
    "gen_sequences = generated_outputs.sequences[:, input_ids.shape[-1]:]\n",
    "\n",
    "# let's stack the logits generated at each step to a tensor and transform\n",
    "# logits to probs\n",
    "probs = torch.stack(generated_outputs.scores, dim=1).softmax(-1)  # -> shape [3, 15, vocab_size]\n",
    "\n",
    "# now we need to collect the probability of the generated token\n",
    "# we need to add a dummy dim in the end to make gather work\n",
    "gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
    "\n",
    "# now we can do all kinds of things with the probs\n",
    "\n",
    "# 1) the probs that exactly those sequences are generated again\n",
    "# those are normally going to be very small\n",
    "unique_prob_per_sequence = gen_probs.prod(-1)\n",
    "\n",
    "\n",
    "# 2) normalize the probs over the three sequences\n",
    "normed_gen_probs = gen_probs / gen_probs.sum(0)\n",
    "assert normed_gen_probs[:, 0].sum() == 1.0, \"probs should be normalized\"\n",
    "\n",
    "# 3) compare normalized probs to each other like in 1)\n",
    "unique_normed_prob_per_sequence = normed_gen_probs.prod(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6271ec5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-88.5249, -90.9243,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [-88.5249, -90.9243,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [-88.5249, -90.9243,     -inf,  ...,     -inf,     -inf,     -inf]]),\n",
       " tensor([[     -inf,      -inf,      -inf,  ...,      -inf,      -inf,\n",
       "               -inf],\n",
       "         [     -inf,      -inf,      -inf,  ...,      -inf,      -inf,\n",
       "               -inf],\n",
       "         [     -inf,      -inf,      -inf,  ...,      -inf,      -inf,\n",
       "          -137.8420]]),\n",
       " tensor([[     -inf,      -inf,      -inf,  ...,      -inf,      -inf,\n",
       "               -inf],\n",
       "         [ -78.2892,  -79.9015,      -inf,  ...,      -inf,      -inf,\n",
       "               -inf],\n",
       "         [     -inf, -124.9655,      -inf,  ...,      -inf,      -inf,\n",
       "          -125.4450]]),\n",
       " tensor([[ -72.6284,  -74.5586,      -inf,  ...,      -inf,      -inf,\n",
       "               -inf],\n",
       "         [     -inf,      -inf,      -inf,  ...,      -inf,      -inf,\n",
       "          -139.2997],\n",
       "         [     -inf, -107.6339,      -inf,  ...,      -inf,      -inf,\n",
       "               -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [-34.0459,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [-83.9293, -85.6923,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]]),\n",
       " tensor([[     -inf, -235.6144,      -inf,  ...,      -inf,      -inf,\n",
       "               -inf],\n",
       "         [     -inf,      -inf,      -inf,  ...,      -inf,      -inf,\n",
       "               -inf],\n",
       "         [     -inf,      -inf,      -inf,  ...,      -inf,      -inf,\n",
       "               -inf]]),\n",
       " tensor([[    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "         [-28.6161, -32.2370,     -inf,  ...,     -inf,     -inf, -33.8384],\n",
       "         [    -inf,     -inf,     -inf,  ...,     -inf,     -inf,     -inf]]),\n",
       " tensor([[     -inf,  -98.6097,      -inf,  ...,      -inf,      -inf,\n",
       "               -inf],\n",
       "         [     -inf,      -inf,      -inf,  ...,      -inf,      -inf,\n",
       "          -119.3600],\n",
       "         [     -inf,      -inf,      -inf,  ...,      -inf,      -inf,\n",
       "               -inf]]),\n",
       " tensor([[     -inf,      -inf,      -inf,  ...,      -inf,      -inf,\n",
       "          -158.7756],\n",
       "         [     -inf, -252.9349,      -inf,  ...,      -inf,      -inf,\n",
       "          -252.4168],\n",
       "         [     -inf,      -inf,      -inf,  ...,      -inf,      -inf,\n",
       "               -inf]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_outputs.scores\n",
    "# sequences - are just the token id sequences of the 2 most probably beams.\n",
    "# sequence_scores - are the cumulative log probabilities of the two most probably beams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7605c273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Today is a nice day, you know, but a lot of it is kind of pointless. That\n",
      "\n",
      "1: Today is a nice day for America. And a great day for the country, too!\n",
      "\n",
      "\n",
      "\n",
      "2: Today is a nice day.\n",
      "\n",
      "I think you guys have to accept that and I just want\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert to text\n",
    "for i in range(3):    \n",
    "    str=tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(generated_outputs.sequences[i]))\n",
    "    print(f\"{i}: {str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac295eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   11,   345,   760,    11,   475,   257,  1256,   286,   340,   318,\n",
       "          1611,   286, 27158,    13,  1320],\n",
       "        [  329,  2253,    13,   843,   257,  1049,  1110,   329,   262,  1499,\n",
       "            11,  1165,     0,   198,   198],\n",
       "        [   13,   198,   198,    40,   892,   345,  3730,   423,   284,  2453,\n",
       "           326,   290,   314,   655,   765]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c306800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1642, 0.0325, 0.3133, 0.1669, 0.1345, 0.0123, 0.1751, 0.9334, 0.0424,\n",
       "         0.4396, 0.0261, 0.9964, 0.0072, 0.3309, 0.0151],\n",
       "        [0.2363, 0.0131, 0.2569, 0.0377, 0.0359, 0.1256, 0.8933, 0.8124, 0.2398,\n",
       "         0.1190, 0.0868, 0.2625, 0.0326, 0.4246, 0.9988],\n",
       "        [0.1301, 0.1795, 0.9926, 0.1283, 0.0274, 0.0518, 0.0357, 0.1315, 0.0175,\n",
       "         0.0089, 0.5324, 0.0231, 0.0265, 0.0128, 0.3363]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecc2ad9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.9582e-12, 4.6304e-09, 1.7850e-14])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_normed_prob_per_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31792dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
