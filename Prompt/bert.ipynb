{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- #\n",
    "# NOTEBOOK MPI EXPERIMENT #\n",
    "# AUTHOR: XIAOYANG SONG   #\n",
    "# ----------------------- #\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tabulate import tabulate\n",
    "sys.path.append('../')\n",
    "from mpi import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from mpi import *\n",
    "\n",
    "dir_path = \"../Dataset/\"\n",
    "log_dir, ckpt_dir = \"../checkpoint/log/\", \"../checkpoint/mpis/\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMultipleChoice.from_pretrained(\"bert-large-cased\")\n",
    "# version = \"bert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(version)\n",
    "# model = BertLMHeadModel.from_pretrained(version)\n",
    "# model_config = dict(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     desc={'family': 'BERT', 'version': version}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dset_lst = ['ocean_15']\n",
    "# ALGO\n",
    "algo_config = {'ll_type':'ans_inv_perp'}\n",
    "# MODEL\n",
    "version = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(version)\n",
    "model = BertLMHeadModel.from_pretrained(version)\n",
    "model_config = dict(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    desc={'family': 'BERT', 'version': version}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ocean_120]_[BERT|bert-base-uncased]_[choice-only]_[ans_inv_perp]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [02:16<00:00,  1.14s/it]\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'utils.QuestionFormatter'>: it's not the same object as utils.QuestionFormatter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/xiaoyangsong/Desktop/DSI Capstone/LLM-Personality-Codebase/Prompt/bert.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert.ipynb#ch0000026?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(filename)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert.ipynb#ch0000026?line=8'>9</a>\u001b[0m mpi \u001b[39m=\u001b[39m run_mpi(dset_config, model_config, algo_config, template_config, log_dir\u001b[39m+\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/DSI%20Capstone/LLM-Personality-Codebase/Prompt/bert.ipynb#ch0000026?line=9'>10</a>\u001b[0m torch\u001b[39m.\u001b[39;49msave(mpi, ckpt_dir\u001b[39m+\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mfilename\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py:379\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py?line=376'>377</a>\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py?line=377'>378</a>\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py?line=378'>379</a>\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py?line=379'>380</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py?line=380'>381</a>\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py:589\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py?line=586'>587</a>\u001b[0m pickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mPickler(data_buf, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py?line=587'>588</a>\u001b[0m pickler\u001b[39m.\u001b[39mpersistent_id \u001b[39m=\u001b[39m persistent_id\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py?line=588'>589</a>\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py?line=589'>590</a>\u001b[0m data_value \u001b[39m=\u001b[39m data_buf\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py?line=590'>591</a>\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(\u001b[39m'\u001b[39m\u001b[39mdata.pkl\u001b[39m\u001b[39m'\u001b[39m, data_value, \u001b[39mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'utils.QuestionFormatter'>: it's not the same object as utils.QuestionFormatter"
     ]
    }
   ],
   "source": [
    "choice_lst = ['choice-only', 'desc-only', 'choice-desc']\n",
    "for dset in dset_lst:\n",
    "    path = dir_path + f\"{dset}.csv\"\n",
    "    dset_config=dict(path_to_dset=path, start_idx=None, end_idx=None)\n",
    "    for choice in choice_lst:\n",
    "        template_config=dict(prompt = MPI_PROMPT,option=CHOICE[choice],choice=CHOICE[choice],shuffle=False)\n",
    "        filename = log_fname(dset, model_config['desc'], choice, algo_config['ll_type'])\n",
    "        print(filename)\n",
    "        mpi = run_mpi(dset_config, model_config, algo_config, template_config, log_dir+f'{filename}.txt', False)\n",
    "        torch.save(mpi, ckpt_dir+f\"{filename}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT-base shuffling order MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"../Dataset/\"\n",
    "log_dir, ckpt_dir = \"../checkpoint/log/BERT-shuffle/\", \"../checkpoint/mpis/BERT-shuffle/\"\n",
    "dset_lst = ['ocean_15']\n",
    "# ALGO\n",
    "algo_config = {'ll_type':'ans_inv_perp'}\n",
    "# MODEL\n",
    "version = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(version)\n",
    "model = BertLMHeadModel.from_pretrained(version)\n",
    "model_config = dict(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    desc={'family': 'BERT', 'version': version}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:16<00:00,  1.10s/it]\n",
      "100%|██████████| 15/15 [00:16<00:00,  1.08s/it]\n",
      "100%|██████████| 15/15 [00:15<00:00,  1.04s/it]\n",
      "100%|██████████| 15/15 [00:16<00:00,  1.09s/it]\n",
      "100%|██████████| 15/15 [00:16<00:00,  1.10s/it]\n",
      "100%|██████████| 15/15 [00:13<00:00,  1.09it/s]\n",
      "100%|██████████| 15/15 [00:13<00:00,  1.09it/s]\n",
      "100%|██████████| 15/15 [00:13<00:00,  1.10it/s]\n",
      "100%|██████████| 15/15 [00:13<00:00,  1.10it/s]\n",
      "100%|██████████| 15/15 [00:14<00:00,  1.07it/s]\n",
      "100%|██████████| 15/15 [00:18<00:00,  1.25s/it]\n",
      "100%|██████████| 15/15 [00:19<00:00,  1.27s/it]\n",
      "100%|██████████| 15/15 [00:18<00:00,  1.26s/it]\n",
      "100%|██████████| 15/15 [00:19<00:00,  1.28s/it]\n",
      "100%|██████████| 15/15 [00:18<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "choice_lst = ['choice-only', 'desc-only', 'choice-desc']\n",
    "num_mc = 5\n",
    "for dset in dset_lst:\n",
    "    path = dir_path + f\"{dset}.csv\"\n",
    "    dset_config=dict(path_to_dset=path, start_idx=None, end_idx=None)\n",
    "    for choice in choice_lst:\n",
    "        # SHUFFLE: ORDER SYMMETRY\n",
    "        for idx in range(num_mc):\n",
    "            template_config=dict(prompt = MPI_PROMPT,option=CHOICE[choice],choice=CHOICE[choice],shuffle=True)\n",
    "            filename = log_fname(dset, model_config['desc'], choice, algo_config['ll_type'])\n",
    "            filename = filename + f\"_[mc{idx}]\"\n",
    "            mpi = run_mpi(dset_config, model_config, algo_config, template_config, log_dir+f'{filename}.txt', False)\n",
    "            torch.save(mpi, ckpt_dir+f\"{filename}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| outputs.last_hidden_state.shape: torch.Size([1, 8, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForCausalLM, AutoConfig\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "config = AutoConfig.from_pretrained(\"roberta-base\")\n",
    "config.is_decoder = True\n",
    "model = RobertaForCausalLM.from_pretrained(\"roberta-base\", config=config)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "prediction_logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 50265])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_logits.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
